{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast # Needed to deserialize lists in CSV file\n",
    "from collections import Counter # Needed for word frequency counting, requires Python 3.10\n",
    "import json # For vocabularies serialization\n",
    "\n",
    "from tqdm import tqdm # Needed to the progress indicators\n",
    "tqdm.pandas(desc='Progress: ')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib # For classifiers serialization\n",
    "\n",
    "from math import log # For log loss score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>discourse_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>[hi, i, isaac, i, go, write, about, natur, lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>[perspect, i, think, natur, landform, i, dont,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>[i, think, natur, landform, life, we, discov, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>[if, life, we, would, i, think, natur, landfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>[thought, form, alien, thought, life]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  discourse_type discourse_effectiveness  \\\n",
       "0           Lead                Adequate   \n",
       "1       Position                Adequate   \n",
       "2          Claim                Adequate   \n",
       "3       Evidence                Adequate   \n",
       "4   Counterclaim                Adequate   \n",
       "\n",
       "                                   discourse_stemmed  \n",
       "0  [hi, i, isaac, i, go, write, about, natur, lan...  \n",
       "1  [perspect, i, think, natur, landform, i, dont,...  \n",
       "2  [i, think, natur, landform, life, we, discov, ...  \n",
       "3  [if, life, we, would, i, think, natur, landfor...  \n",
       "4              [thought, form, alien, thought, life]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.read_csv(\"../input/feedback-prize-effectiveness/train_preproc_v4.csv\", \\\n",
    "                       converters={\"discourse_stemmed\" : ast.literal_eval})\n",
    "\n",
    "# We will only retain the \"discourse_type\", \"discourse_effectiveness\", and \"discourse_stemmed\" columns for the training\n",
    "input_df = input_df[[\"discourse_type\", \"discourse_effectiveness\", \"discourse_stemmed\"]]\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>7097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>3405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claim</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Effective</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Effective</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>6064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evidence</td>\n",
       "      <td>Effective</td>\n",
       "      <td>2885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Evidence</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>1244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lead</td>\n",
       "      <td>Effective</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lead</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>2784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Position</td>\n",
       "      <td>Effective</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Position</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rebuttal</td>\n",
       "      <td>Adequate</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rebuttal</td>\n",
       "      <td>Effective</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rebuttal</td>\n",
       "      <td>Ineffective</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          discourse_type discourse_effectiveness  count\n",
       "0                  Claim                Adequate   7097\n",
       "1                  Claim               Effective   3405\n",
       "2                  Claim             Ineffective   1475\n",
       "3   Concluding Statement                Adequate   1945\n",
       "4   Concluding Statement               Effective    825\n",
       "5   Concluding Statement             Ineffective    581\n",
       "6           Counterclaim                Adequate   1150\n",
       "7           Counterclaim               Effective    418\n",
       "8           Counterclaim             Ineffective    205\n",
       "9               Evidence                Adequate   6064\n",
       "10              Evidence               Effective   2885\n",
       "11              Evidence             Ineffective   3156\n",
       "12                  Lead                Adequate   1244\n",
       "13                  Lead               Effective    683\n",
       "14                  Lead             Ineffective    364\n",
       "15              Position                Adequate   2784\n",
       "16              Position               Effective    770\n",
       "17              Position             Ineffective    470\n",
       "18              Rebuttal                Adequate    693\n",
       "19              Rebuttal               Effective    340\n",
       "20              Rebuttal             Ineffective    211"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how our learning samples are distributed across different discourse types and effectiveness\n",
    "sample_counts = input_df.groupby([\"discourse_type\", \"discourse_effectiveness\"]).count()\n",
    "sample_counts.rename(columns={\"discourse_stemmed\": \"count\"}, inplace=True) # Rename the last column into \"count\"\n",
    "sample_counts.reset_index(inplace=True) # Best do this after the grouping\n",
    "sample_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adequate': 20977, 'Effective': 9326, 'Ineffective': 6462}\n",
      "{'Claim': {'Adequate': 7097, 'Effective': 3405, 'Ineffective': 1475}, 'Concluding Statement': {'Adequate': 1945, 'Effective': 825, 'Ineffective': 581}, 'Counterclaim': {'Adequate': 1150, 'Effective': 418, 'Ineffective': 205}, 'Evidence': {'Adequate': 6064, 'Effective': 2885, 'Ineffective': 3156}, 'Lead': {'Adequate': 1244, 'Effective': 683, 'Ineffective': 364}, 'Position': {'Adequate': 2784, 'Effective': 770, 'Ineffective': 470}, 'Rebuttal': {'Adequate': 693, 'Effective': 340, 'Ineffective': 211}}\n"
     ]
    }
   ],
   "source": [
    "# Let's convert above data frame into a python dictionary\n",
    "sample_counts_by_type = {} # Retains the breakup by dicourse type\n",
    "sample_counts_typeless = {} # Doesn't retain the breakup by dicourse type\n",
    "\n",
    "for discourse_type, discourse_effectiveness, count in sample_counts.itertuples(index=False):\n",
    "    if discourse_type not in sample_counts_by_type:\n",
    "        sample_counts_by_type[discourse_type] = {}\n",
    "    if discourse_effectiveness not in sample_counts_typeless:\n",
    "        sample_counts_typeless[discourse_effectiveness] = 0\n",
    "        \n",
    "    sample_counts_by_type[discourse_type][discourse_effectiveness] = count\n",
    "    sample_counts_typeless[discourse_effectiveness] += count\n",
    "\n",
    "print(sample_counts_typeless)\n",
    "print(sample_counts_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Representative (Sample-Balanced) Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As number of training samples for different discourse types and effectiveness is rather different,\n",
    "# we will prepare dataframes with balanced number of samples\n",
    "\n",
    "# Data frames with unbalanced samples\n",
    "X_unbalanced, y_unbalanced = input_df[[\"discourse_type\", \"discourse_stemmed\"]], input_df[\"discourse_effectiveness\"]\n",
    "\n",
    "# Data frames with samples balanced by effectiveness\n",
    "\n",
    "# Split input_df by discourse_effectiveness\n",
    "adequate_df = input_df[input_df[\"discourse_effectiveness\"] == \"Adequate\"]\n",
    "effective_df = input_df[input_df[\"discourse_effectiveness\"] == \"Effective\"]\n",
    "ineffective_df = input_df[input_df[\"discourse_effectiveness\"] == \"Ineffective\"]\n",
    "\n",
    "# Retain the largest possible number of samples while keeping them balanced by discourse_effectiveness\n",
    "min_samples = min([len(adequate_df), len(effective_df), len(ineffective_df)])\n",
    "adequate_df_sampled = adequate_df.sample(n = min_samples, random_state = 32167)\n",
    "effective_df_sampled = effective_df.sample(n = min_samples, random_state = 32167)\n",
    "ineffective_df_sampled = ineffective_df.sample(n = min_samples, random_state = 32167)\n",
    "\n",
    "effectiveness_balanced_df = pd.concat([adequate_df_sampled, effective_df_sampled, ineffective_df_sampled]).sample(frac = 1, random_state = 32167)\n",
    "X_effectiveness_balanced, y_effectiveness_balanced = effectiveness_balanced_df[[\"discourse_type\", \"discourse_stemmed\"]], effectiveness_balanced_df[\"discourse_effectiveness\"]\n",
    "\n",
    "# Data frames with samples unbalanced by effectivenss but split by time, and balanced by type and effectivenes\n",
    "discourse_types = [\"Claim\", \"Concluding Statement\", \"Counterclaim\", \"Evidence\", \"Lead\", \"Position\", \"Rebuttal\"]\n",
    "\n",
    "unbalance_type_split_dfs = {}\n",
    "X_unbalanced_type_split, y_unbalanced_type_split = {}, {}\n",
    "\n",
    "type_effectiveness_balanced_dfs = {}\n",
    "X_type_and_effectivess_balanced, y_type_and_effectivess_balanced = {}, {}\n",
    "\n",
    "for dt in discourse_types: # For every discourse type\n",
    "    \n",
    "    # Split adequate_df, effective_df, ineffective_df by discourse_type\n",
    "    unbalance_type_split_dfs[dt] = {}\n",
    "    unbalance_type_split_dfs[dt][\"Adequate\"] = adequate_df[adequate_df[\"discourse_type\"] == dt]\n",
    "    unbalance_type_split_dfs[dt][\"Effective\"] = effective_df[effective_df[\"discourse_type\"] == dt]\n",
    "    unbalance_type_split_dfs[dt][\"Ineffective\"] = ineffective_df[ineffective_df[\"discourse_type\"] == dt]\n",
    "    \n",
    "    # Again, retain the largest possible number of samples while keeping them balanced by discourse type and effectiveness\n",
    "    min_samples = min([len(unbalance_type_split_dfs[dt][\"Adequate\"]), len(unbalance_type_split_dfs[dt][\"Effective\"]), len(unbalance_type_split_dfs[dt][\"Ineffective\"])])\n",
    "    type_effectiveness_balanced_dfs[dt] = {}\n",
    "    type_effectiveness_balanced_dfs[dt][\"Adequate\"] = unbalance_type_split_dfs[dt][\"Adequate\"].sample(n = min_samples, random_state = 32167)\n",
    "    type_effectiveness_balanced_dfs[dt][\"Effective\"] = unbalance_type_split_dfs[dt][\"Effective\"].sample(n = min_samples, random_state = 32167)\n",
    "    type_effectiveness_balanced_dfs[dt][\"Ineffective\"] = unbalance_type_split_dfs[dt][\"Ineffective\"].sample(n = min_samples, random_state = 32167)\n",
    "\n",
    "    unbalance_type_split_dfs[dt] = pd.concat([unbalance_type_split_dfs[dt][\"Adequate\"], unbalance_type_split_dfs[dt][\"Effective\"], unbalance_type_split_dfs[dt][\"Ineffective\"]]).sample(frac = 1, random_state = 32167)\n",
    "    X_unbalanced_type_split[dt] = unbalance_type_split_dfs[dt][[\"discourse_type\", \"discourse_stemmed\"]]\n",
    "    y_unbalanced_type_split[dt] = unbalance_type_split_dfs[dt][\"discourse_effectiveness\"]\n",
    "    \n",
    "    type_effectiveness_balanced_dfs[dt] = pd.concat([type_effectiveness_balanced_dfs[dt][\"Adequate\"], type_effectiveness_balanced_dfs[dt][\"Effective\"], type_effectiveness_balanced_dfs[dt][\"Ineffective\"]]).sample(frac = 1, random_state = 32167)\n",
    "    X_type_and_effectivess_balanced[dt] = type_effectiveness_balanced_dfs[dt][[\"discourse_type\", \"discourse_stemmed\"]]\n",
    "    y_type_and_effectivess_balanced[dt] = type_effectiveness_balanced_dfs[dt][\"discourse_effectiveness\"]\n",
    "\n",
    "# Outcomes of this cell:\n",
    "#    X_unbalanced, y_unbalanced\n",
    "#    X_unbalanced_type_split, y_unbalanced_type_split\n",
    "#    X_effectiveness_balanced, y_effectiveness_balanced\n",
    "#    X_type_and_effectivess_balanced, y_type_and_effectivess_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabularies Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the unbalanced vocabulary: \n",
      "   - Adequate: 344851\n",
      "   - Effective: 287093\n",
      "   - Ineffective: 150948\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Here we will prepare the vocabularies with word occurences counts from the previously prepared datasets,\n",
    "    and convert their \"discourse_stemmed\" columns into feature vectors\n",
    "\"\"\"\n",
    "vocabs = {\"unbalanced\": {}, \"unbalanced_type_split\": {}, \"effectiveness_balanced\": {}, \"type_and_effectivess_balanced\": {}}\n",
    "\n",
    "unbalanced_df = pd.concat([X_unbalanced, y_unbalanced], axis=1) # Recover the input DF\n",
    "\n",
    "# Concatenate all the discourse_stemmed for every discourse_effectiveness\n",
    "unbalanced_df = unbalanced_df[[\"discourse_effectiveness\", \"discourse_stemmed\"]].groupby(\"discourse_effectiveness\").agg(sum)\n",
    "unbalanced_df.rename(columns={\"discourse_stemmed\": \"all_words\"}, inplace=True) # Rename the last column into \"top100_word_count\"\n",
    "unbalanced_df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Total words in the unbalanced vocabulary: \")\n",
    "\n",
    "# For every discourse_effectiveness\n",
    "for row in unbalanced_df.itertuples():\n",
    "    discourse_effectiveness, all_words = row[1], row[2]\n",
    "    vocabs[\"unbalanced\"][discourse_effectiveness] = dict(Counter(all_words)) # Count all the words for that discourse_effectiveness\n",
    "\n",
    "    print(f\"   - {discourse_effectiveness}: {Counter(all_words).total()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the effectiveness balanced vocabulary: \n",
      "   - Adequate: 108581\n",
      "   - Effective: 197355\n",
      "   - Ineffective: 150948\n"
     ]
    }
   ],
   "source": [
    "# Now we will do the exactly same thing, but for the effectiveness balanced dataset\n",
    "effectiveness_balanced_df = pd.concat([X_effectiveness_balanced, y_effectiveness_balanced], axis=1) # Recover the input DF\n",
    "\n",
    "# Concatenate all the discourse_stemmed for every discourse_effectiveness\n",
    "effectiveness_balanced_df = effectiveness_balanced_df[[\"discourse_effectiveness\", \"discourse_stemmed\"]].groupby(\"discourse_effectiveness\").agg(sum)\n",
    "effectiveness_balanced_df.rename(columns={\"discourse_stemmed\": \"all_words\"}, inplace=True) # Rename the last column into \"top100_word_count\"\n",
    "effectiveness_balanced_df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Total words in the effectiveness balanced vocabulary: \")\n",
    "\n",
    "# For every discourse_effectiveness\n",
    "for row in effectiveness_balanced_df.itertuples():\n",
    "    discourse_effectiveness, all_words = row[1], row[2]\n",
    "    vocabs[\"effectiveness_balanced\"][discourse_effectiveness] = dict(Counter(all_words)) # Count all the words for that discourse_effectiveness\n",
    "\n",
    "    print(f\"   - {discourse_effectiveness}: {Counter(all_words).total()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the Claim effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 58079\n",
      "   - Effective: 33059\n",
      "   - Ineffective: 12717\n",
      "Total words in the Concluding Statement effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 46120\n",
      "   - Effective: 37532\n",
      "   - Ineffective: 9586\n",
      "Total words in the Counterclaim effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 12638\n",
      "   - Effective: 6279\n",
      "   - Ineffective: 1969\n",
      "Total words in the Evidence effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 171021\n",
      "   - Effective: 167051\n",
      "   - Ineffective: 113522\n",
      "Total words in the Lead effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 25740\n",
      "   - Effective: 26531\n",
      "   - Ineffective: 6488\n",
      "Total words in the Position effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 23334\n",
      "   - Effective: 9631\n",
      "   - Ineffective: 4507\n",
      "Total words in the Rebuttal effectiveness unbalanced type split vocabulary: \n",
      "   - Adequate: 7919\n",
      "   - Effective: 7010\n",
      "   - Ineffective: 2159\n"
     ]
    }
   ],
   "source": [
    "# The same thing, but for the effectivess unbalanced type split dataset\n",
    "for dt in discourse_types:\n",
    "\n",
    "    # Recover the input DF\n",
    "    effectivenss_unbalance_type_split_dfs = pd.concat([X_unbalanced_type_split[dt], y_unbalanced_type_split[dt]], axis=1)\n",
    "    \n",
    "    # Concatenate all the discourse_stemmed for every discourse_effectiveness\n",
    "    effectivenss_unbalance_type_split_dfs = effectivenss_unbalance_type_split_dfs[[\"discourse_effectiveness\", \"discourse_stemmed\"]].groupby(\"discourse_effectiveness\").agg(sum)\n",
    "    effectivenss_unbalance_type_split_dfs.rename(columns={\"discourse_stemmed\": \"all_words\"}, inplace=True) # Rename the last column into \"top100_word_count\"\n",
    "    effectivenss_unbalance_type_split_dfs.reset_index(inplace=True)\n",
    "    \n",
    "    print(f\"Total words in the {dt} effectiveness unbalanced type split vocabulary: \")\n",
    "    vocabs[\"unbalanced_type_split\"][dt] = {}\n",
    "    \n",
    "    # For every discourse_effectiveness\n",
    "    for row in effectivenss_unbalance_type_split_dfs.itertuples():\n",
    "        discourse_effectiveness, all_words = row[1], row[2]\n",
    "        vocabs[\"unbalanced_type_split\"][dt][discourse_effectiveness] = dict(Counter(all_words)) # Count all the words for that discourse_effectiveness\n",
    "\n",
    "        print(f\"   - {discourse_effectiveness}: {Counter(all_words).total()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the Claim effectiveness balanced vocabulary: \n",
      "   - Adequate: 12158\n",
      "   - Effective: 14163\n",
      "   - Ineffective: 12717\n",
      "Total words in the Concluding Statement effectiveness balanced vocabulary: \n",
      "   - Adequate: 13984\n",
      "   - Effective: 26462\n",
      "   - Ineffective: 9586\n",
      "Total words in the Counterclaim effectiveness balanced vocabulary: \n",
      "   - Adequate: 2433\n",
      "   - Effective: 3225\n",
      "   - Ineffective: 1969\n",
      "Total words in the Evidence effectiveness balanced vocabulary: \n",
      "   - Adequate: 82579\n",
      "   - Effective: 167051\n",
      "   - Ineffective: 104355\n",
      "Total words in the Lead effectiveness balanced vocabulary: \n",
      "   - Adequate: 7573\n",
      "   - Effective: 13979\n",
      "   - Ineffective: 6488\n",
      "Total words in the Position effectiveness balanced vocabulary: \n",
      "   - Adequate: 3913\n",
      "   - Effective: 6012\n",
      "   - Ineffective: 4507\n",
      "Total words in the Rebuttal effectiveness balanced vocabulary: \n",
      "   - Adequate: 2348\n",
      "   - Effective: 4301\n",
      "   - Ineffective: 2159\n"
     ]
    }
   ],
   "source": [
    "# And again, the same thing, but for type and effectivess balanced dataset\n",
    "for dt in discourse_types:\n",
    "    # Recover the input DF\n",
    "    type_and_effectivess_balanced_df = pd.concat([X_type_and_effectivess_balanced[dt], y_type_and_effectivess_balanced[dt]], axis=1)\n",
    "    \n",
    "    # Concatenate all the discourse_stemmed for every discourse_effectiveness\n",
    "    type_and_effectivess_balanced_df = type_and_effectivess_balanced_df[[\"discourse_effectiveness\", \"discourse_stemmed\"]].groupby(\"discourse_effectiveness\").agg(sum)\n",
    "    type_and_effectivess_balanced_df.rename(columns={\"discourse_stemmed\": \"all_words\"}, inplace=True) # Rename the last column into \"top100_word_count\"\n",
    "    type_and_effectivess_balanced_df.reset_index(inplace=True)\n",
    "    \n",
    "    print(f\"Total words in the {dt} effectiveness balanced vocabulary: \")\n",
    "    vocabs[\"type_and_effectivess_balanced\"][dt] = {}\n",
    "    \n",
    "    # For every discourse_effectiveness\n",
    "    for row in type_and_effectivess_balanced_df.itertuples():\n",
    "        discourse_effectiveness, all_words = row[1], row[2]\n",
    "        vocabs[\"type_and_effectivess_balanced\"][dt][discourse_effectiveness] = dict(Counter(all_words)) # Count all the words for that discourse_effectiveness\n",
    "\n",
    "        print(f\"   - {discourse_effectiveness}: {Counter(all_words).total()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaly we will write out the vocabularies in a file\n",
    "with open('./data/vocab_v2.json', 'w', encoding ='utf8') as json_file:\n",
    "    json.dump(vocabs, json_file)\n",
    "    \n",
    "# We then immidiately reload the vocabs from file to be able to catch the potential serialization issues faster\n",
    "with open('./data/vocab_v2.json', 'r', encoding ='utf8') as json_file:\n",
    "    vocabs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████████████████████████████████████| 36765/36765 [00:00<00:00, 198189.58it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████| 36765/36765 [00:00<00:00, 197040.86it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████| 36765/36765 [00:00<00:00, 198188.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((29412, 4), (7353, 4), (29412,), (7353,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Next we need to convert \"discourse_stemmed\" columns into feature vector of the form:\n",
    "        (1.0, ac, ec, ic), where ac, ec, ic  are sum of counts of every stem counts in the\n",
    "        adequate, effective, ineffective vocabularies.\n",
    "\"\"\"\n",
    "\n",
    "X_unbalanced_feat = X_unbalanced\n",
    "\n",
    "X_unbalanced_feat[\"bias\"] = 1.0\n",
    "X_unbalanced_feat[\"ac\"] = X_unbalanced_feat[\"discourse_stemmed\"].progress_apply( \\\n",
    "                lambda x: sum([(vocabs[\"unbalanced\"][\"Adequate\"][w] if w in vocabs[\"unbalanced\"][\"Adequate\"] else 0) for w in x]))\n",
    "X_unbalanced_feat[\"ec\"] = X_unbalanced_feat[\"discourse_stemmed\"].progress_apply( \\\n",
    "                lambda x: sum([(vocabs[\"unbalanced\"][\"Effective\"][w] if w in vocabs[\"unbalanced\"][\"Effective\"] else 0) for w in x]))\n",
    "X_unbalanced_feat[\"ic\"] = X_unbalanced_feat[\"discourse_stemmed\"].progress_apply( \\\n",
    "                lambda x: sum([(vocabs[\"unbalanced\"][\"Ineffective\"][w] if w in vocabs[\"unbalanced\"][\"Ineffective\"] else 0) for w in x]))\n",
    "\n",
    "# Now prepare numpy feature vectors and split the datasets into training and validation dataset\n",
    "X_unbalanced_feat = X_unbalanced_feat[[\"bias\", \"ac\", \"ec\", \"ic\"]]\n",
    "X_unbalanced_feat, y_unbalanced_feat = X_unbalanced_feat.to_numpy(), y_unbalanced.to_numpy()\n",
    "\n",
    "X_unbalanced_feat_train, X_unbalanced_feat_val, y_unbalanced_feat_train, y_unbalanced_feat_val = \\\n",
    "    train_test_split(X_unbalanced_feat, y_unbalanced_feat, test_size = 0.20, random_state = 32167)\n",
    "\n",
    "X_unbalanced_feat_train.shape, X_unbalanced_feat_val.shape, y_unbalanced_feat_train.shape, y_unbalanced_feat_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████████████████████████████████████| 19386/19386 [00:00<00:00, 164022.31it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████| 19386/19386 [00:00<00:00, 160591.99it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████| 19386/19386 [00:00<00:00, 159323.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((15508, 4), (3878, 4), (15508,), (3878,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will do the same, but for the dataset balanced by effectiveness\n",
    "X_effectiveness_balanced_feat = X_effectiveness_balanced\n",
    "\n",
    "X_effectiveness_balanced_feat[\"bias\"] = 1.0\n",
    "X_effectiveness_balanced_feat[\"ac\"] = X_effectiveness_balanced_feat[\"discourse_stemmed\"].progress_apply( \\\n",
    "                lambda x: sum([(vocabs[\"effectiveness_balanced\"][\"Adequate\"][w] if w in vocabs[\"effectiveness_balanced\"][\"Adequate\"] else 0) for w in x]))\n",
    "X_effectiveness_balanced_feat[\"ec\"] = X_effectiveness_balanced_feat[\"discourse_stemmed\"].progress_apply( \\\n",
    "                lambda x: sum([(vocabs[\"effectiveness_balanced\"][\"Effective\"][w] if w in vocabs[\"effectiveness_balanced\"][\"Effective\"] else 0) for w in x]))\n",
    "X_effectiveness_balanced_feat[\"ic\"] = X_effectiveness_balanced_feat[\"discourse_stemmed\"].progress_apply( \\\n",
    "                lambda x: sum([(vocabs[\"effectiveness_balanced\"][\"Ineffective\"][w] if w in vocabs[\"effectiveness_balanced\"][\"Ineffective\"] else 0) for w in x]))\n",
    "\n",
    "# Now prepare numpy feature vectors and split the datasets into training and validation dataset\n",
    "X_effectiveness_balanced_feat = X_effectiveness_balanced_feat[[\"bias\", \"ac\", \"ec\", \"ic\"]]\n",
    "X_effectiveness_balanced_feat, y_effectiveness_balanced_feat = X_effectiveness_balanced_feat.to_numpy(), y_effectiveness_balanced.to_numpy()\n",
    "\n",
    "X_effectiveness_balanced_feat_train, X_effectiveness_balanced_feat_val, y_effectiveness_balanced_feat_train, y_effectiveness_balanced_feat_val = \\\n",
    "    train_test_split(X_effectiveness_balanced_feat, y_effectiveness_balanced_feat, test_size = 0.20, random_state = 32167)\n",
    "\n",
    "X_effectiveness_balanced_feat_train.shape, X_effectiveness_balanced_feat_val.shape, y_effectiveness_balanced_feat_train.shape, y_effectiveness_balanced_feat_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████████████████████████████████████| 11977/11977 [00:00<00:00, 286031.72it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████| 11977/11977 [00:00<00:00, 292892.59it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████| 11977/11977 [00:00<00:00, 300210.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim (9581, 4) (2396, 4) (9581,) (2396,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 3351/3351 [00:00<00:00, 129226.97it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 3351/3351 [00:00<00:00, 124437.69it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 3351/3351 [00:00<00:00, 120093.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concluding Statement (2680, 4) (671, 4) (2680,) (671,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1773/1773 [00:00<00:00, 253970.19it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1773/1773 [00:00<00:00, 222217.27it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1773/1773 [00:00<00:00, 254109.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counterclaim (1418, 4) (355, 4) (1418,) (355,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████████████████████████████████████| 12105/12105 [00:00<00:00, 100952.32it/s]\n",
      "Progress: 100%|███████████████████████████████████████████████████████████████| 12105/12105 [00:00<00:00, 99073.99it/s]\n",
      "Progress: 100%|███████████████████████████████████████████████████████████████| 12105/12105 [00:00<00:00, 96327.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence (9684, 4) (2421, 4) (9684,) (2421,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 2291/2291 [00:00<00:00, 135260.13it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 2291/2291 [00:00<00:00, 135121.29it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 2291/2291 [00:00<00:00, 135011.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead (1832, 4) (459, 4) (1832,) (459,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 4024/4024 [00:00<00:00, 288210.23it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 4024/4024 [00:00<00:00, 288195.47it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 4024/4024 [00:00<00:00, 268965.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position (3219, 4) (805, 4) (3219,) (805,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1244/1244 [00:00<00:00, 207885.34it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1244/1244 [00:00<00:00, 206748.59it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1244/1244 [00:00<00:00, 207885.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuttal (995, 4) (249, 4) (995,) (249,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we will do the same, but for the dataset unbalanced balanced by the discourse effectiveness and split by type\n",
    "X_unbalanced_type_split_feat, y_unbalanced_type_split_feat = {}, {}\n",
    "X_unbalanced_type_split_feat_train, X_unbalanced_type_split_feat_val = {}, {}\n",
    "y_unbalanced_type_split_feat_train, y_unbalanced_type_split_feat_val = {}, {}\n",
    "\n",
    "for dt in discourse_types:\n",
    "    X_unbalanced_type_split_feat[dt] = X_unbalanced_type_split[dt]\n",
    "\n",
    "    X_unbalanced_type_split_feat[dt][\"bias\"] = 1.0\n",
    "    X_unbalanced_type_split_feat[dt][\"ac\"] = X_unbalanced_type_split_feat[dt][\"discourse_stemmed\"].progress_apply( \\\n",
    "                    lambda x: sum([(vocabs[\"unbalanced_type_split\"][dt][\"Adequate\"][w] if w in vocabs[\"unbalanced_type_split\"][dt][\"Adequate\"] else 0) for w in x]))\n",
    "    X_unbalanced_type_split_feat[dt][\"ec\"] = X_unbalanced_type_split_feat[dt][\"discourse_stemmed\"].progress_apply( \\\n",
    "                    lambda x: sum([(vocabs[\"unbalanced_type_split\"][dt][\"Effective\"][w] if w in vocabs[\"unbalanced_type_split\"][dt][\"Effective\"] else 0) for w in x]))\n",
    "    X_unbalanced_type_split_feat[dt][\"ic\"] = X_unbalanced_type_split_feat[dt][\"discourse_stemmed\"].progress_apply( \\\n",
    "                    lambda x: sum([(vocabs[\"unbalanced_type_split\"][dt][\"Ineffective\"][w] if w in vocabs[\"unbalanced_type_split\"][dt][\"Ineffective\"] else 0) for w in x]))\n",
    "    \n",
    "    # Now prepare numpy feature vectors and split the datasets into training and validation dataset\n",
    "    X_unbalanced_type_split_feat[dt] = X_unbalanced_type_split_feat[dt][[\"bias\", \"ac\", \"ec\", \"ic\"]]\n",
    "    X_unbalanced_type_split_feat[dt], y_unbalanced_type_split_feat[dt] = X_unbalanced_type_split_feat[dt].to_numpy(), y_unbalanced_type_split[dt].to_numpy()\n",
    "\n",
    "    X_unbalanced_type_split_feat_train[dt], X_unbalanced_type_split_feat_val[dt], y_unbalanced_type_split_feat_train[dt], y_unbalanced_type_split_feat_val[dt] = \\\n",
    "        train_test_split(X_unbalanced_type_split_feat[dt], y_unbalanced_type_split_feat[dt], test_size = 0.20, random_state = 32167)\n",
    "\n",
    "    print(dt, X_unbalanced_type_split_feat_train[dt].shape, X_unbalanced_type_split_feat_val[dt].shape, y_unbalanced_type_split_feat_train[dt].shape, y_unbalanced_type_split_feat_val[dt].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 4425/4425 [00:00<00:00, 295792.48it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 4425/4425 [00:00<00:00, 277289.16it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 4425/4425 [00:00<00:00, 295806.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim (3540, 4) (885, 4) (3540,) (885,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1743/1743 [00:00<00:00, 116514.02it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1743/1743 [00:00<00:00, 124827.92it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1743/1743 [00:00<00:00, 116250.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concluding Statement (1394, 4) (349, 4) (1394,) (349,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████████████████████████████████████████| 615/615 [00:00<00:00, 206707.02it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████████| 615/615 [00:00<00:00, 154156.28it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████████| 615/615 [00:00<00:00, 205603.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counterclaim (492, 4) (123, 4) (492,) (123,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|█████████████████████████████████████████████████████████████████| 8655/8655 [00:00<00:00, 90396.06it/s]\n",
      "Progress: 100%|█████████████████████████████████████████████████████████████████| 8655/8655 [00:00<00:00, 85055.33it/s]\n",
      "Progress: 100%|█████████████████████████████████████████████████████████████████| 8655/8655 [00:00<00:00, 89466.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence (6924, 4) (1731, 4) (6924,) (1731,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 136310.82it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 121645.06it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 136872.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead (873, 4) (219, 4) (873,) (219,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1410/1410 [00:00<00:00, 282789.11it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1410/1410 [00:00<00:00, 235625.67it/s]\n",
      "Progress: 100%|████████████████████████████████████████████████████████████████| 1410/1410 [00:00<00:00, 235635.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position (1128, 4) (282, 4) (1128,) (282,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████████████████████████████████████████████████████████████| 633/633 [00:00<00:00, 211553.34it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████████| 633/633 [00:00<00:00, 211553.34it/s]\n",
      "Progress: 100%|██████████████████████████████████████████████████████████████████| 633/633 [00:00<00:00, 211587.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuttal (506, 4) (127, 4) (506,) (127,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally we will do the same, but for the dataset balanced by the discourse type and effectiveness\n",
    "X_type_and_effectivess_balanced_feat, y_type_and_effectivess_balanced_feat = {}, {}\n",
    "X_type_and_effectivess_balanced_feat_train, X_type_and_effectivess_balanced_feat_val = {}, {}\n",
    "y_type_and_effectivess_balanced_feat_train, y_type_and_effectivess_balanced_feat_val = {}, {}\n",
    "\n",
    "for dt in discourse_types:\n",
    "    X_type_and_effectivess_balanced_feat[dt] = X_type_and_effectivess_balanced[dt]\n",
    "\n",
    "    X_type_and_effectivess_balanced_feat[dt][\"bias\"] = 1.0\n",
    "    X_type_and_effectivess_balanced_feat[dt][\"ac\"] = X_type_and_effectivess_balanced_feat[dt][\"discourse_stemmed\"].progress_apply( \\\n",
    "                    lambda x: sum([(vocabs[\"type_and_effectivess_balanced\"][dt][\"Adequate\"][w] if w in vocabs[\"type_and_effectivess_balanced\"][dt][\"Adequate\"] else 0) for w in x]))\n",
    "    X_type_and_effectivess_balanced_feat[dt][\"ec\"] = X_type_and_effectivess_balanced_feat[dt][\"discourse_stemmed\"].progress_apply( \\\n",
    "                    lambda x: sum([(vocabs[\"type_and_effectivess_balanced\"][dt][\"Effective\"][w] if w in vocabs[\"type_and_effectivess_balanced\"][dt][\"Effective\"] else 0) for w in x]))\n",
    "    X_type_and_effectivess_balanced_feat[dt][\"ic\"] = X_type_and_effectivess_balanced_feat[dt][\"discourse_stemmed\"].progress_apply( \\\n",
    "                    lambda x: sum([(vocabs[\"type_and_effectivess_balanced\"][dt][\"Ineffective\"][w] if w in vocabs[\"type_and_effectivess_balanced\"][dt][\"Ineffective\"] else 0) for w in x]))\n",
    "\n",
    "    # Now prepare numpy feature vectors and split the datasets into training and validation dataset\n",
    "    X_type_and_effectivess_balanced_feat[dt] = X_type_and_effectivess_balanced_feat[dt][[\"bias\", \"ac\", \"ec\", \"ic\"]]\n",
    "    X_type_and_effectivess_balanced_feat[dt], y_type_and_effectivess_balanced_feat[dt] = X_type_and_effectivess_balanced_feat[dt].to_numpy(), y_type_and_effectivess_balanced[dt].to_numpy()\n",
    "\n",
    "    X_type_and_effectivess_balanced_feat_train[dt], X_type_and_effectivess_balanced_feat_val[dt], y_type_and_effectivess_balanced_feat_train[dt], y_type_and_effectivess_balanced_feat_val[dt] = \\\n",
    "        train_test_split(X_type_and_effectivess_balanced_feat[dt], y_type_and_effectivess_balanced_feat[dt], test_size = 0.20, random_state = 32167)\n",
    "\n",
    "    print(dt, X_type_and_effectivess_balanced_feat_train[dt].shape, X_type_and_effectivess_balanced_feat_val[dt].shape, y_type_and_effectivess_balanced_feat_train[dt].shape, y_type_and_effectivess_balanced_feat_val[dt].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5368557051543588"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LogisticRegression classifier on the unbalanced training data\n",
    "unbalanced_clf = LogisticRegression(multi_class='multinomial', random_state = 32167).fit( \\\n",
    "                    X_unbalanced_feat_train, y_unbalanced_feat_train)\n",
    "\n",
    "# Check the accuracy on the training data\n",
    "unbalanced_clf.score(X_unbalanced_feat_train, y_unbalanced_feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4913592984266185"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LogisticRegression classifier on the effectivenees balanced training data\n",
    "effectiveness_balanced_clf = LogisticRegression(multi_class='multinomial', random_state = 32167).fit( \\\n",
    "                    X_effectiveness_balanced_feat_train, y_effectiveness_balanced_feat_train)\n",
    "\n",
    "# Check the accuracy on the training data\n",
    "effectiveness_balanced_clf.score(X_effectiveness_balanced_feat_train, y_effectiveness_balanced_feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim classifier accuracy: 0.618828932261768\n",
      "Concluding Statement classifier accuracy: 0.6309701492537313\n",
      "Counterclaim classifier accuracy: 0.6995768688293371\n",
      "Evidence classifier accuracy: 0.4847170590665014\n",
      "Lead classifier accuracy: 0.6124454148471615\n",
      "Position classifier accuracy: 0.7104690897794346\n",
      "Rebuttal classifier accuracy: 0.6150753768844222\n"
     ]
    }
   ],
   "source": [
    "# Train the LogisticRegression classifiers on the effectivenees unbalanced and type split training data\n",
    "effectiveness_unbalanced_type_split_clfs = {}\n",
    "\n",
    "for dt in discourse_types:\n",
    "    effectiveness_unbalanced_type_split_clfs[dt] = LogisticRegression(multi_class='multinomial', max_iter = 10000, random_state = 32167).fit( \\\n",
    "                    X_unbalanced_type_split_feat_train[dt], y_unbalanced_type_split_feat_train[dt])\n",
    "    \n",
    "    # Check the accuracy on the training data\n",
    "    print(dt, \"classifier accuracy:\", effectiveness_unbalanced_type_split_clfs[dt].score(X_unbalanced_type_split_feat_train[dt], y_unbalanced_type_split_feat_train[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim classifier accuracy: 0.5152542372881356\n",
      "Concluding Statement classifier accuracy: 0.6119081779053085\n",
      "Counterclaim classifier accuracy: 0.6239837398373984\n",
      "Evidence classifier accuracy: 0.4904679376083189\n",
      "Lead classifier accuracy: 0.586483390607102\n",
      "Position classifier accuracy: 0.5851063829787234\n",
      "Rebuttal classifier accuracy: 0.6304347826086957\n"
     ]
    }
   ],
   "source": [
    "# Train the LogisticRegression classifiers on the effectivenees and type balanced training data\n",
    "type_and_effectiveness_balanced_clfs = {}\n",
    "\n",
    "for dt in discourse_types:\n",
    "    type_and_effectiveness_balanced_clfs[dt] = LogisticRegression(multi_class='multinomial', random_state = 32167).fit( \\\n",
    "                    X_type_and_effectivess_balanced_feat_train[dt], y_type_and_effectivess_balanced_feat_train[dt])\n",
    "    \n",
    "    # Check the accuracy on the training data\n",
    "    print(dt, \"classifier accuracy:\", type_and_effectiveness_balanced_clfs[dt].score(X_type_and_effectivess_balanced_feat_train[dt], y_type_and_effectivess_balanced_feat_train[dt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save our trained classifiers into a file\n",
    "classifiers = {\"unbalanced\" : unbalanced_clf, \"effectiveness_balanced\" : effectiveness_balanced_clf, \\\n",
    "               \"unbalanced_type_split\": effectiveness_unbalanced_type_split_clfs, \"type_and_effectivess_balanced\" : type_and_effectiveness_balanced_clfs}\n",
    "\n",
    "joblib.dump(classifiers, './data/log_regression_classifiers_v2.joblib')\n",
    "\n",
    "# We immidiately load them from the file, to make sure the deserialization was succesfull\n",
    "classifiers = joblib.load('./data/log_regression_classifiers_v2.joblib') \n",
    "\n",
    "unbalanced_clf = classifiers[\"unbalanced\"]\n",
    "effectiveness_balanced_clf = classifiers[\"effectiveness_balanced\"]\n",
    "effectiveness_unbalanced_type_split_clfs = classifiers[\"unbalanced_type_split\"]\n",
    "type_and_effectiveness_balanced_clfs = classifiers[\"type_and_effectivess_balanced\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5344757241942064"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the unbalanced validation data\n",
    "y_unbalanced_feat_pred = unbalanced_clf.predict_proba(X_unbalanced_feat_val)\n",
    "\n",
    "# Check the accuracy on the unbalanced validation data\n",
    "unbalanced_clf.score(X_unbalanced_feat_val, y_unbalanced_feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.502578648788035"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the effectiveness balanced validation data\n",
    "y_effectiveness_balanced_feat_pred = effectiveness_balanced_clf.predict_proba(X_effectiveness_balanced_feat_val)\n",
    "\n",
    "# Also make a prediction for the unbalanced validation data using the \n",
    "y_unbalanced_feat_pred_v2 = effectiveness_balanced_clf.predict_proba(X_unbalanced_feat_val)\n",
    "\n",
    "# Check the accuracy on the effectiveness balanced validation data\n",
    "effectiveness_balanced_clf.score(X_effectiveness_balanced_feat_val, y_effectiveness_balanced_feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813953488372093"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy of the effectiveness balanced classifier on the unbalanced validation data\n",
    "effectiveness_balanced_clf.score(X_unbalanced_feat_val, y_unbalanced_feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim classifier accuracy: 0.6181135225375626\n",
      "Concluding Statement classifier accuracy: 0.6199701937406855\n",
      "Counterclaim classifier accuracy: 0.6985915492957746\n",
      "Evidence classifier accuracy: 0.4861627426683189\n",
      "Lead classifier accuracy: 0.6034858387799564\n",
      "Position classifier accuracy: 0.715527950310559\n",
      "Rebuttal classifier accuracy: 0.6546184738955824\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the effectiveness unbalanced and type split validation data\n",
    "y_effectiveness_unbalanced_type_split_feat_pred = {}\n",
    "\n",
    "for dt in discourse_types:\n",
    "    y_effectiveness_unbalanced_type_split_feat_pred[dt] = effectiveness_unbalanced_type_split_clfs[dt].predict_proba(X_unbalanced_type_split_feat_val[dt])\n",
    "\n",
    "    # Check the accuracy on the type and effectiveness balanced validation data\n",
    "    print(dt, \"classifier accuracy:\", effectiveness_unbalanced_type_split_clfs[dt].score(X_unbalanced_type_split_feat_val[dt], y_unbalanced_type_split_feat_val[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim classifier accuracy: 0.5525423728813559\n",
      "Concluding Statement classifier accuracy: 0.5587392550143266\n",
      "Counterclaim classifier accuracy: 0.6422764227642277\n",
      "Evidence classifier accuracy: 0.49451184286539573\n",
      "Lead classifier accuracy: 0.5570776255707762\n",
      "Position classifier accuracy: 0.599290780141844\n",
      "Rebuttal classifier accuracy: 0.6614173228346457\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the type and effectiveness balanced validation data\n",
    "y_type_and_effectiveness_balanced_feat_pred = {}\n",
    "\n",
    "for dt in discourse_types:\n",
    "    y_type_and_effectiveness_balanced_feat_pred[dt] = type_and_effectiveness_balanced_clfs[dt].predict_proba(X_type_and_effectivess_balanced_feat_val[dt])\n",
    "\n",
    "    # Check the accuracy on the type and effectiveness balanced validation data\n",
    "    print(dt, \"classifier accuracy:\", type_and_effectiveness_balanced_clfs[dt].score(X_type_and_effectivess_balanced_feat_val[dt], y_type_and_effectivess_balanced_feat_val[dt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0050887953386125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the classification score according to logloss used by kaggle\n",
    "\n",
    "# Calculates the log_loss\n",
    "def log_loss(classes, y_pred, y_true):\n",
    "\n",
    "    log_loss = 0\n",
    "    for i in range(len(y_pred)): # For every prediction\n",
    "        for j in range(len(classes)): # For every class\n",
    "            if y_true[i] == classes[j]: # If this is the class we were suppose to predict\n",
    "                log_loss += log(y_pred[i][j])\n",
    "\n",
    "    return - log_loss / len(y_pred)\n",
    "\n",
    "# Check the log loss of the unbalanced classifier predictions on the unbalanced validation data\n",
    "log_loss(unbalanced_clf.classes_, y_unbalanced_feat_pred, y_unbalanced_feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992421308286923"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the log loss of the effectiveness balanced classifier predictions on the effectiveness balanced validation data\n",
    "log_loss(effectiveness_balanced_clf.classes_, y_effectiveness_balanced_feat_pred, y_effectiveness_balanced_feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6410932524055752"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the log loss of the effectiveness balanced classifier predictions on the unbalanced validation data\n",
    "log_loss(effectiveness_balanced_clf.classes_, y_unbalanced_feat_pred_v2, y_unbalanced_feat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Log Loss:  0.8677560164203595\n",
      "Concluding Statement Log Loss:  0.8255081967781289\n",
      "Counterclaim Log Loss:  0.7657091506063284\n",
      "Evidence Log Loss:  1.0038510831357623\n",
      "Lead Log Loss:  0.8638954601493577\n",
      "Position Log Loss:  0.7231432980088076\n",
      "Rebuttal Log Loss:  0.8179144209620549\n",
      "Log loss accross all predictions:  0.8860153205605533\n"
     ]
    }
   ],
   "source": [
    "# Check the log loss of the effectiveness unbalanced type split classifiers predictions\n",
    "\n",
    "total_log_loss, total_predictions = 0, 0\n",
    "for dt in discourse_types:\n",
    "    ll = log_loss(effectiveness_unbalanced_type_split_clfs[dt].classes_, y_effectiveness_unbalanced_type_split_feat_pred[dt], y_unbalanced_type_split_feat_val[dt])\n",
    "    total_log_loss += (ll * len(y_effectiveness_unbalanced_type_split_feat_pred[dt]))\n",
    "    total_predictions += len(y_effectiveness_unbalanced_type_split_feat_pred[dt])\n",
    "    print(dt, \"Log Loss: \", ll)\n",
    "    \n",
    "print(\"Log loss accross all predictions: \", total_log_loss / total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Log Loss:  0.9777727675222816\n",
      "Concluding Statement Log Loss:  0.9138941093147707\n",
      "Counterclaim Log Loss:  0.7559243775557885\n",
      "Evidence Log Loss:  0.9702979163488463\n",
      "Lead Log Loss:  0.9130097639329984\n",
      "Position Log Loss:  0.9175974071069756\n",
      "Rebuttal Log Loss:  0.837826481372549\n",
      "Log loss accross all predictions:  0.9477819981935136\n"
     ]
    }
   ],
   "source": [
    "# Check the log loss of the effectiveness and type balanced classifier predictions on the effectiveness and type balanced validation data\n",
    "\n",
    "total_log_loss, total_predictions = 0, 0\n",
    "for dt in discourse_types:\n",
    "    ll = log_loss(type_and_effectiveness_balanced_clfs[dt].classes_, y_type_and_effectiveness_balanced_feat_pred[dt], y_type_and_effectivess_balanced_feat_val[dt])\n",
    "    total_log_loss += (ll * len(y_type_and_effectiveness_balanced_feat_pred[dt]))\n",
    "    total_predictions += len(y_type_and_effectiveness_balanced_feat_pred[dt])\n",
    "    print(dt, \"Log Loss: \", ll)\n",
    "    \n",
    "print(\"Log loss accross all predictions: \", total_log_loss / total_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results are pretty bad (around the statistics based prediction baseline, although significantly better than guessing at random), which is not very surprising for the clasifier that simple\n",
    "* Numbers for accuracy and log loss don't agree on classifiers trained on which datasets are best, so we'll try to submit all of them to kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
